# -*- coding: utf-8 -*-
"""AssignmentEC8_Chatharina.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YfTZZpgwdG_dvaVgbFUF0JDxaaQRJXm8

# Install & Load Library
"""

# load scikitplot untuk visualisasi metrik
import streamlit as st
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.metrics import (
    roc_auc_score, roc_curve, accuracy_score, precision_score, recall_score, f1_score,
    confusion_matrix, classification_report
)
from sklearn.dummy import DummyClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier

from sklearn.experimental import enable_halving_search_cv
from sklearn.model_selection import HalvingRandomSearchCV
from sklearn.inspection import PartialDependenceDisplay

pd.set_option('display.max_columns', 100)

"""# Load Dataset"""

# load dataset ke raw_data
st.sidebar.header("⚙️ Pengaturan")
url_default = "https://raw.githubusercontent.com/hadimaster65555/dataset_for_teaching/refs/heads/main/dataset/bank_churn_dataset_2/Churn_Modelling.csv"
URL = url_default

uploaded = st.sidebar.file_uploader("Upload CSV", type=["csv"])
test_size = st.sidebar.slider("Test size", 0.1, 0.4, 0.2, 0.05)
seed = st.sidebar.number_input("Random state (SEED)", min_value=0, value=42, step=1)
thr = st.sidebar.slider("Threshold prediksi (positif = churn)", 0.01, 0.99, 0.50, 0.01)

@st.cache_data
def load_df(file):
    if file is not None:
        return pd.read_csv(file)
    return pd.read_csv(URL)

df = load_df(uploaded)

"""#Eksplorasi Data

##Data Understanding
"""

df.shape

"""Ada beberapa yang sebaiknya disesuaikan:
- ID/indeks (RowNumber, CustomerId) → string (identifier, bukan nilai numerik).
- Kategorikal (Surname, Geography, Gender) → category (hemat memori & jelas untuk encoding).
- Biner (HasCrCard, IsActiveMember) → boolean (True/False).
"""

# salin dulu agar aman
df_typed = df.copy()

# 1) Kolom identifier -> string (bukan numerik)
df_typed["RowNumber"]  = df_typed["RowNumber"].astype(str)
df_typed["CustomerId"] = df_typed["CustomerId"].astype(str)

# 2) Kolom kategorikal -> category
for col in ["Surname", "Geography", "Gender"]:
  df_typed[col] = df_typed[col].astype("category")

# 3) Kolom biner -> boolean (nullable-safe)
for col in ["HasCrCard", "IsActiveMember"]:
  df_typed[col] = df_typed[col].map({1: True, 0: False}).astype("boolean")

# 4) Downcast numerik untuk hemat memori
# target Exited tetap int (umum untuk modeling)
num_int_cols = ["CreditScore", "Age", "Tenure", "NumOfProducts", "Exited"]
for col in num_int_cols:
  df_typed[col] = pd.to_numeric(df_typed[col], downcast="integer")

num_float_cols = ["Balance", "EstimatedSalary"]
for col in num_float_cols:
  df_typed[col] = pd.to_numeric(df_typed[col], downcast="float")

# 5) Cek hasil
st.write("Dtypes sebelum:\n", df.dtypes, "\n")
st.write("Dtypes sesudah:\n", df_typed.dtypes, "\n")

# 6) Ringkas memori hemat vs asli
mem_before = df.memory_usage(deep=True).sum() / (1024**2)
mem_after  = df_typed.memory_usage(deep=True).sum() / (1024**2)
st.write(f"Memori sebelum: {mem_before:.2f} MB")
st.write(f"Memori sesudah: {mem_after:.2f} MB")

# 7) Tinjau beberapa baris
st.write("\nContoh data (kolom kunci):")
st.write(df_typed[["RowNumber","CustomerId","Surname","Geography","Gender","HasCrCard","IsActiveMember","Exited"]].head())

#Periksa nilai yang hilang (jumlah & persentase)
missing_summary = pd.DataFrame({
  "Jumlah_NA": df.isnull().sum(),
  "Persentase_NA(%)": (df.isnull().mean() * 100).round(2)
})

st.write("Ringkasan Missing Values:")
st.write(missing_summary)

# Opsional: filter hanya kolom yang punya missing values
missing_only = missing_summary[missing_summary["Jumlah_NA"] > 0]
if missing_only.empty:
  st.write("\n Tidak ada nilai yang hilang di dataset ini.")
else:
  st.write("\n Kolom dengan nilai hilang:")
  st.write(missing_only)

# Pilih hanya kolom numerik
numeric_cols = df.select_dtypes(include=["int64", "float64"]).columns

# Hitung statistik deskriptif
stat_desc = pd.DataFrame({
  "Mean": df[numeric_cols].mean(),
  "Median": df[numeric_cols].median(),
  "Std_Dev": df[numeric_cols].std(),
  "Q1": df[numeric_cols].quantile(0.25),
  "Q3": df[numeric_cols].quantile(0.75)
})

st.write("Statistik Deskriptif Variabel Numerik:")
st.dataframe(stat_desc)

# Pilih kolom kategorikal (tipe object atau category)
cat_cols = df.select_dtypes(include=["object"]).columns

st.write("Kolom kategorikal:", list(cat_cols), "\n")

# Loop untuk setiap kolom kategorikal
for col in cat_cols:
  st.write(f"Distribusi kategori untuk: {col}")
  st.write(df[col].value_counts())
  st.write("\nPersentase:")
  st.write((df[col].value_counts(normalize=True) * 100).round(2))
  st.write("-" * 40)

# 1. Pisahkan fitur (X) dan target (y)
X = df.drop(["Exited"], axis=1)
y = df["Exited"]

# 2. Split data menjadi train dan test set (stratify agar distribusi Churn seimbang di kedua set)
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=test_size,
    stratify=y,
    random_state=1000
)

# 3. Gabungkan kembali X_train dengan y_train untuk eksplorasi
train_data = X_train.copy()
train_data["Exited"] = y_train

# Create a temporary DataFrame for plotting the target distribution
# This ensures the 'Exited' column is present for visualization
plot_data = X_train.copy()
plot_data['Exited'] = y_train

fig, ax = plt.subplots(figsize=(6,4))
sns.countplot(x="Exited", data=plot_data, palette="viridis") # Use plot_data
plt.title("Distribusi Target Exited di Data Training")
plt.xlabel("Exited")
plt.ylabel("Jumlah")
st.pyplot(fig)

# Persentase churn
churn_rate = plot_data['Exited'].value_counts(normalize=True) * 100 # Use plot_data
st.write("Persentase Exited:")
st.write(churn_rate)

# pakai dataframe training yang sudah ada
# kalau kamu sudah punya X_train, y_train:
df_corr = X_train.copy()
target_col = "Exited" # Changed Churn to Exited
df_corr[target_col] = y_train

# 1) Ambil hanya fitur numerik (agar korelasi valid)
df_num = df_corr.select_dtypes(include=[np.number])

# (opsional) kalau mau Spearman (lebih robust utk non-linear), set method="spearman"
corr = df_num.corr(method="spearman")

# 2) Mask untuk menampilkan hanya segitiga bawah
mask = np.triu(np.ones_like(corr, dtype=bool), k=1)   # sembunyikan segitiga atas

# 3) Plot heatmap
fig, ax = plt.subplots(figsize=(10,8))
sns.heatmap(
    corr,
    mask=mask,
    cmap="RdBu_r",
    vmin=-1, vmax=1,
    annot=True, fmt=".2f",
    linewidths=.5,
    cbar=True,
    square=True
)
plt.title("Lower-Triangular Correlation Heatmap (Spearman)")
plt.tight_layout()
st.pyplot(fig)

# 4) (opsional) daftar korelasi fitur terhadap target, urut absolut
target_corr = (
    corr[target_col]
    .drop(labels=[target_col])
    .reindex(corr.columns, fill_value=np.nan)
    .dropna()
    .sort_values(key=np.abs, ascending=False)
)
st.write("\nTop korelasi fitur vs target:")
st.write(target_corr.head(15).to_string())

"""##Categorical Data vs Churn"""

# Setup data
target_col = "Exited"
df_train = X_train.copy()
df_train[target_col] = y_train

# pilih kolom kategorikal (kecuali target)
categorical_cols = df_train.select_dtypes(include=["object","category","bool"]).columns.tolist()
categorical_cols = [c for c in categorical_cols if c != target_col]

# Parameter agar cepat
MAX_CAT = 12
TOP_N  = 6
BATCH  = 4

def plot_cat_vs_churn(ax, df, cat_col, target, top_n=TOP_N):
    # ambil top kategori jika terlalu banyak
    vc = df[cat_col].value_counts(dropna=False)
    cats = vc.index.tolist()
    if len(cats) > MAX_CAT:
        cats = vc.head(top_n).index.tolist()
        df = df[df[cat_col].isin(cats)]

    # tabel proporsi churn per kategori
    tab = pd.crosstab(df[cat_col], df[target], normalize='index')
    # pastikan kolom 0/1 ada
    for cls in [0,1]:
        if cls not in tab.columns:
            tab[cls] = 0.0
    tab = tab[[0,1]].rename(columns={0:"No",1:"Yes"})  # biar label jelas
    tab = tab.reset_index().melt(id_vars=cat_col, var_name="Churn", value_name="prop")

    # plot
    groups = tab["Churn"].unique().tolist()
    x = np.arange(len(tab[cat_col].unique()))
    width = 0.38

    # data untuk dua bar (No/Yes)
    tab_no  = tab[tab["Churn"]==groups[0]]
    tab_yes = tab[tab["Churn"]==groups[1]] if len(groups)>1 else None

    ax.bar(x - width/2, tab_no["prop"].values, width, label=groups[0])
    if tab_yes is not None:
        ax.bar(x + width/2, tab_yes["prop"].values, width, label=groups[1])

    # anotasi angka
    for xi, v in zip(x - width/2, tab_no["prop"].values):
        ax.text(xi, v, f"{v:.2f}", ha="center", va="bottom", fontsize=9)
    if tab_yes is not None:
        for xi, v in zip(x + width/2, tab_yes["prop"].values):
            ax.text(xi, v, f"{v:.2f}", ha="center", va="bottom", fontsize=9)

    ax.set_xticks(x)
    ax.set_xticklabels(tab_no[cat_col].astype(str), rotation=0)
    ax.set_ylim(0, 1)
    ax.set_ylabel("Proporsi")
    ax.set_title(f"{cat_col} vs {target}")
    ax.legend(title="Churn")

# Render per-batch
for start in range(0, len(categorical_cols), BATCH):
    cols_batch = categorical_cols[start:start+BATCH]
    n = len(cols_batch)
    ncols = 2 if n > 1 else 1
    nrows = int(np.ceil(n / ncols))

    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(6*ncols, 4*nrows))
    axes = np.array(axes).reshape(-1) if isinstance(axes, np.ndarray) else np.array([axes])

    for ax, col in zip(axes, cols_batch):
        plot_cat_vs_churn(ax, df_train, col, target_col)

    # kosongkan sisa axes jika tidak terpakai
    for k in range(len(cols_batch), len(axes)):
        fig.delaxes(axes[k])

    plt.tight_layout()
    st.pyplot(fig)
    plt.close(fig)

# Setup data
target_col = "Exited" # Changed Churn to Exited
df_train = X_train.copy()
df_train[target_col] = y_train

# Ambil semua kolom numerik (kecuali target)
numeric_cols = df_train.select_dtypes(include=[np.number]).columns.tolist()
numeric_cols = [c for c in numeric_cols if c != target_col]

# Parameter batch
BATCH = 4  # berapa fitur per figure (grid)

def plot_box_vs_churn(ax, df, num_col, target):
    sns.boxplot(
        data=df,
        x=target,
        y=num_col,
        ax=ax,
        palette="Set2",
        showfliers=False  # biar gak terlalu banyak titik ekstrem
    )
    ax.set_title(f"{num_col} vs {target}")
    ax.set_xlabel(target)
    ax.set_ylabel(num_col)

# Render per-batch
for start in range(0, len(numeric_cols), BATCH):
    cols_batch = numeric_cols[start:start+BATCH]
    n = len(cols_batch)
    ncols = 2 if n > 1 else 1
    nrows = int(np.ceil(n / ncols))

    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(6*ncols, 4*nrows))
    axes = np.array(axes).reshape(-1) if isinstance(axes, np.ndarray) else np.array([axes])

    for ax, col in zip(axes, cols_batch):
        plot_box_vs_churn(ax, df_train, col, target_col)

    # kosongkan sisa axes jika tidak terpakai
    for k in range(len(cols_batch), len(axes)):
        fig.delaxes(axes[k])

    plt.tight_layout()
    st.pyplot(fig)
    plt.close(fig)

"""#Visualisasi Data"""

# Tentukan nama target yang tersedia
target_col = "Exited" if "Exited" in df.columns else ("Churn" if "Churn" in df.columns else None)
assert target_col is not None, f"Kolom target tidak ditemukan. Kolom yang ada: {list(df.columns)}"

# Buat label ramah-baca
df = df.copy()
df["Churn_label"] = df[target_col].map({0: "No", 1: "Yes"}).astype("category")

# Boxplot: EstimatedSalary vs Churn
plt.figure(figsize=(6,4))
sns.boxplot(data=df, x="Churn_label", y="EstimatedSalary", palette="viridis", showfliers=False)
plt.title("EstimatedSalary vs Churn")
plt.xlabel("Churn")
plt.ylabel("EstimatedSalary")
plt.tight_layout()
st.pyplot(fig)

# Pastikan tipe target benar
df["Exited"] = df["Exited"].astype(int)
df["Exited_label"] = df["Exited"].map({0: "Stay", 1: "Churn"})

vars_num = ["Age", "CreditScore", "Balance"]
palette = {"Stay": "#4c78a8", "Churn": "#f58518"}

# HISTOGRAM
fig, axes = plt.subplots(1, 3, figsize=(18, 4))
for ax, col in zip(axes, vars_num):
  sns.histplot(
    data=df, x=col, hue="Exited_label",
    bins=30, stat="density", common_norm=False,
    element="step", kde=True, alpha=0.25, palette=palette, ax=ax
  )
  ax.set_title(f"Histogram {col} by Churn")
  ax.set_xlabel(col); ax.set_ylabel("Density")
  ax.grid(True, linestyle="--", alpha=0.3)
plt.tight_layout()
st.pyplot(fig)

# BOX PLOT (berdampingan, 1 baris 3 kolom)
fig, axes = plt.subplots(1, 3, figsize=(18, 4))
for ax, col in zip(axes, vars_num):
  sns.boxplot(
    data=df, x="Exited_label", y=col, palette=palette, ax=ax
  )
  ax.set_title(f"Box Plot {col} by Churn")
  ax.set_xlabel("Churn"); ax.set_ylabel(col)
  ax.grid(True, axis="y", linestyle="--", alpha=0.3)
plt.tight_layout()
st.pyplot(fig)

# SCATTER
#- y = Exited dengan jitter kecil supaya titik tidak menumpuk di 0/1
rng = np.random.default_rng(42)
jitter = (rng.random(len(df)) - 0.5) * 0.2  # ~±0.1
y_jitter = df["Exited"] + jitter

fig, axes = plt.subplots(1, 3, figsize=(18, 4))
for ax, col in zip(axes, vars_num):
  # warnai sesuai churn
  colors = df["Exited"].map({0: palette["Stay"], 1: palette["Churn"]})
  ax.scatter(df[col], y_jitter, s=12, alpha=0.35, c=colors)
  ax.set_title(f"Scatter {col} vs Churn (dengan jitter)")
  ax.set_xlabel(col); ax.set_ylabel("Exited (0=Stay, 1=Churn)")
  ax.set_yticks([0, 1]); ax.set_yticklabels(["Stay", "Churn"])
  ax.grid(True, linestyle="--", alpha=0.3)
plt.tight_layout()
st.pyplot(fig)

# Tambahkan label churn
df["Exited_label"] = df["Exited"].map({0: "Stay", 1: "Churn"})

# Buat figure
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# --- Grafik NumOfProducts ---
sns.countplot(
  data=df,
  x="NumOfProducts",
  hue="Exited_label",
  palette={"Stay": "#4c78a8", "Churn": "#f58518"},
  ax=axes[0]
)
axes[0].set_title("Distribusi NumOfProducts berdasarkan Status Churn", fontsize=12)
axes[0].set_xlabel("Jumlah Produk yang Dimiliki", fontsize=10)
axes[0].set_ylabel("Jumlah Nasabah", fontsize=10)
axes[0].tick_params(axis='x', rotation=0)
axes[0].grid(axis="y", linestyle="--", alpha=0.3)
# Tambah keterangan
axes[0].text(
  0.5, -0.25,
  "Keterangan: 1 = punya 1 produk, 2 = punya 2 produk, 3 = punya 3 produk, 4 = punya 4 produk",
  ha="center", va="top", transform=axes[0].transAxes, fontsize=9
)

# Grafik Tenure
sns.countplot(
  data=df,
  x="Tenure",
  hue="Exited_label",
  palette={"Stay": "#4c78a8", "Churn": "#f58518"},
  ax=axes[1]
)
axes[1].set_title("Distribusi Tenure berdasarkan Status Churn", fontsize=12)
axes[1].set_xlabel("Lama Menjadi Nasabah (tahun)", fontsize=10)
axes[1].set_ylabel("Jumlah Nasabah", fontsize=10)
axes[1].tick_params(axis='x', rotation=0)
axes[1].grid(axis="y", linestyle="--", alpha=0.3)
# Tambah keterangan
axes[1].text(
  0.5, -0.25,
  "Keterangan: angka 0–10 menunjukkan lama menjadi nasabah (dalam tahun)",
  ha="center", va="top", transform=axes[1].transAxes, fontsize=9
)

plt.tight_layout()
st.pyplot(fig)

"""#Rekayasa Fitur"""

# Pastikan kolom target dan kolom yang dipakai bertipe numerik yang konsisten
df["Exited"] = df["Exited"].astype(int)
df["IsActiveMember"] = df["IsActiveMember"].astype(int)
df["HasCrCard"] = df["HasCrCard"].astype(int)

df_fe = df.copy()
df_fe["Age_x_Active"] = df_fe["Age"] * df_fe["IsActiveMember"]

# Rasio saldo terhadap gaji (kemampuan menahan saldo relatif ke income)
df_fe["Balance_to_Salary"] = (df_fe["Balance"] / (df_fe["EstimatedSalary"] + 1e-6)).replace([np.inf, -np.inf], 0)

# Tenure per umur (indikasi loyalitas relatif terhadap usia)
df_fe["Tenure_per_Age"] = df_fe["Tenure"] / (df_fe["Age"] + 1e-6)

# Interaksi produk × aktif (apakah pengguna aktif dengan banyak produk)
df_fe["Products_x_Active"] = df_fe["NumOfProducts"] * df_fe["IsActiveMember"]

# Biner usia senior (sering berbeda perilaku churn)
df_fe["IsSenior_55plus"] = (df_fe["Age"] >= 55).astype(int)

# Bucket umur (untuk model yang sensitif non-linearitas sederhana)
age_bins = [0, 25, 35, 45, 55, 65, 120]
age_labels = ["<=25", "26-35", "36-45", "46-55", "56-65", "65+"]
df_fe["Age_Bin"] = pd.cut(df_fe["Age"], bins=age_bins, labels=age_labels, right=True, include_lowest=True)

# Skor kredit rendah/tinggi (threshold contoh—silakan tuning)
df_fe["LowCreditScore"] = (df_fe["CreditScore"] < 600).astype(int)
df_fe["HighCreditScore"] = (df_fe["CreditScore"] >= 750).astype(int)

# Cenderung “wealthy” (saldo besar) — threshold contoh (tuning sesuai EDA)
df_fe["HighBalance"] = (df_fe["Balance"] > 150000).astype(int)

new_cols = [
  "Age_x_Active", "Balance_to_Salary", "Tenure_per_Age",
  "Products_x_Active", "IsSenior_55plus", "Age_Bin",
  "LowCreditScore", "HighCreditScore", "HighBalance"
]
st.write("Kolom baru:", new_cols)
st.write("\nPreview fitur baru:")
df_fe[new_cols + ["Exited"]].head()

cols_categorical = ["Geography", "Gender", "Age_Bin"]
df_model = pd.get_dummies(df_fe.drop(columns=["RowNumber","CustomerId","Surname"]), columns=cols_categorical, drop_first=True)

st.write("\nShape siap modeling:", df_model.shape)
st.write("Kolom siap modeling (cuplikan):")
df_model.columns[:25]

"""#Pemrosesan Awal Data"""

#Laporan missing values
def missing_report(df):
  rep = pd.DataFrame({
    "n_missing": df.isna().sum(),
    "pct_missing": (df.isna().mean() * 100).round(2)
  }).sort_values("pct_missing", ascending=False)
  return rep

st.write("Ringkasan Missing Values:")
missing_report(df).head()

# Drop kolom dengan missing berlebihan
# Atur threshold sesuai kebutuhan (mis. drop jika >60% NA)
DROP_THRESHOLD = 0.60
missing_summary = missing_report(df)
missing_summary['pct_missing_decimal'] = missing_summary['pct_missing'] / 100 # Calculate percentage as decimal

cols_to_drop = missing_summary.query("pct_missing_decimal > @DROP_THRESHOLD").index.tolist()
if cols_to_drop:
  st.write("\nDropping columns with excessive missing:", cols_to_drop)
  df = df.drop(columns=cols_to_drop)
else:
  st.write("\nNo columns to drop based on the missing value threshold.")

"""Threshold 60% dipakai supaya:
  - Jaga fitur penting – Kolom yang masih punya >40% data jangan langsung dibuang karena mungkin berguna.
  - Kurangi noise – Kalau missing >60%, hasil imputasi rawan bikin model jelek.
  - Seimbang – Nggak terlalu ketat (buang banyak fitur) tapi nggak terlalu longgar (fitur penuh tebakan).
"""

# Siapkan target & fitur (buang ID yang tak dipakai modeling)
y = df["Exited"].astype(int)
X = df.drop(columns=["Exited", "RowNumber", "CustomerId", "Surname"], errors="ignore")

"""Supaya model hanya belajar dari informasi relevan dan tidak “ngintip” target atau ID unik yang tidak ada di data baru:
- y = df["Exited"].astype(int) → Ambil kolom Exited sebagai target (label churn) dan ubah ke tipe integer biar cocok untuk model ML.
- X = df.drop([...]) → Buang kolom yang tidak relevan atau bisa bikin bias:
  - RowNumber, CustomerId, Surname → hanya ID/identitas, tidak punya pengaruh prediksi.
  - Exited → sudah jadi target, jadi harus dihapus dari fitur.
"""

# Train/Test split (hindari leakage saat imputasi)
X_train, X_test, y_train, y_test = train_test_split(
  X, y, test_size=test_size, random_state=42, stratify=y)

# Deteksi tipe kolom
numeric_features = X_train.select_dtypes(include=["int64", "float64", "int32", "float32"]).columns.tolist()
categorical_features = X_train.select_dtypes(include=["object", "category", "bool"]).columns.tolist()

st.write("\nNumeric features:", numeric_features)
st.write("Categorical features:", categorical_features)

# Pipeline imputasi
# Numerik: median (robust terhadap outlier)
numeric_pipeline = Pipeline(steps=[
  ("imputer", SimpleImputer(strategy="median"))
])

"""Pipeline imputasi ini dipakai untuk kolom numerik. SimpleImputer(strategy="median") → Mengisi nilai yang hilang (missing) dengan median, karena median tidak terpengaruh ekstrem/outlier seperti mean. Dibungkus di Pipeline supaya langkah ini bisa otomatis jalan bareng preprocessing lain, dan aman saat fit hanya di data train (hindari data leakage)."""

# Kategorikal: modus; jika kolom seluruhnya NA, SimpleImputer akan isi dengan string 'missing' jika strategy='most_frequent' tidak valid.
# Untuk jaga-jaga, kita gunakan 'constant' -> 'Unknown' agar selalu aman.
categorical_pipeline = Pipeline(steps=[
  ("imputer", SimpleImputer(strategy="constant", fill_value="Unknown")),
  ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=False))
])

"""- Tujuan → Menangani kolom kategorikal yang punya missing value.
-  SimpleImputer(strategy="constant", fill_value="Unknown") → Diisi "Unknown" supaya:
  - Selalu valid walau seluruh kolom isinya NA (kalau pakai most_frequent, akan error kalau semua NA).
  - Tidak “mengarang” nilai kategori yang bisa bikin bias, tapi memberi label netral "Unknown.
  - Disclaimer: ini aman karena data kita tidak punya hubungan hierarkis antar kategori (misal kota–provinsi–negara), sehingga tidak perlu verifikasi relasi antar kolom.
- OneHotEncoder(handle_unknown="ignore") → Ubah kategori jadi angka biner; handle_unknown="ignore" mencegah error jika di data test muncul kategori baru.

"""

# Siapkan target & fitur (buang ID yang tak dipakai modeling)
y = df["Exited"].astype(int)
X = df.drop(columns=["Exited", "RowNumber", "CustomerId", "Surname"], errors="ignore")

# Train/Test split (hindari leakage saat imputasi)
X_train, X_test, y_train, y_test = train_test_split(
  X, y, test_size=test_size, random_state=42, stratify=y)

# Deteksi tipe kolom
numeric_features = X_train.select_dtypes(include=["int64", "float64", "int32", "float32"]).columns.tolist()
categorical_features = X_train.select_dtypes(include=["object", "category", "bool"]).columns.tolist()

st.write("\nNumeric features:", numeric_features)
st.write("Categorical features:", categorical_features)

# Pipeline imputasi
# Numerik: median (robust terhadap outlier)
numeric_pipeline = Pipeline(steps=[
  ("imputer", SimpleImputer(strategy="median"))
])

# Kategorikal: modus; jika kolom seluruhnya NA, SimpleImputer akan isi dengan string 'missing' jika strategy='most_frequent' tidak valid.
# Untuk jaga-jaga, kita gunakan 'constant' -> 'Unknown' agar selalu aman.
categorical_pipeline = Pipeline(steps=[
  ("imputer", SimpleImputer(strategy="constant", fill_value="Unknown")),
  ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=False))
])

# Gabungkan
preprocess = ColumnTransformer(
  transformers=[
    ("num", numeric_pipeline, numeric_features),
    ("cat", categorical_pipeline, categorical_features),
  ],
  remainder="drop"
)

"""- numeric_pipeline → Untuk kolom numerik, isi nilai hilang dengan median (lebih tahan outlier dibanding mean).
- categorical_pipeline → Untuk kolom kategorikal, isi nilai hilang dengan Unknown (aman walau semua NA dan tidak bikin bias karena tidak ada hierarki kategori), lalu ubah jadi variabel biner lewat OneHotEncoder.
- ColumnTransformer → Menggabungkan dua pipeline tadi, sehingga:
  - Semua preprocessing dilakukan terstruktur dan konsisten.
  - Bisa fit hanya di data train lalu otomatis transform data test (hindari data leakage).
- remainder="drop" → Kolom yang tidak disebut akan dibuang supaya hanya fitur yang relevan masuk ke model.
"""

# Fit di train, transform train & test
X_train_pre = preprocess.fit_transform(X_train)
X_test_pre  = preprocess.transform(X_test)

st.write("\nShape sebelum preprocessing:", X_train.shape, X_test.shape)
st.write("Shape sesudah preprocessing (siap ke model):", X_train_pre.shape, X_test_pre.shape)

"""- Preprocess.fit_transform(X_train) → Latih langkah preprocessing (imputasi, scaling, encoding) hanya di data train lalu langsung ubah datanya.
- Preprocess.transform(X_test) → Terapkan preprocessing yang sudah dipelajari dari train ke data test (supaya tidak bocor info dari test ke model).
- Print shape → Mengecek jumlah fitur sebelum & sesudah preprocessing. Setelah preprocessing biasanya jumlah kolom bertambah karena OneHotEncoding membuat kolom baru untuk tiap kategori.
"""

def get_feature_names(preprocess, num_cols, cat_cols):
  num_names = num_cols
  cat_names = []
  ohe = preprocess.named_transformers_["cat"].named_steps["onehot"]
  ohe_feature_names = ohe.get_feature_names_out(cat_cols)
  cat_names.extend(ohe_feature_names.tolist())
  return num_names + cat_names

feature_names = get_feature_names(preprocess, numeric_features, categorical_features)
st.write("\nContoh 20 fitur hasil preprocessing:")
feature_names[:20]

"""- Fungsi get_feature_names → Mengambil kembali nama fitur asli setelah preprocessing, termasuk hasil OneHotEncoding.
- num_names = num_cols → Nama fitur numerik tidak berubah.
- ohe.get_feature_names_out(cat_cols) → Mengambil nama kolom baru dari encoding kategorikal (misalnya Geography_France, Geography_Germany).
- Gabungkan nama numerik & kategorikal agar urut sesuai posisi di data hasil transformasi.
- feature_names[:20] → Menampilkan contoh 20 fitur pertama untuk cek apakah semua nama fitur sudah benar dan mudah diinterpretasi.

##Skala atau normalisasi fitur numerik
"""

numeric_features = X_train.select_dtypes(include=["int64", "float64"]).columns.tolist()
categorical_features = X_train.select_dtypes(include=["object", "category", "bool"]).columns.tolist()

# Pipeline numerik: imputasi median + scaling
numeric_pipeline = Pipeline(steps=[
  ("imputer", SimpleImputer(strategy="median")),
  ("scaler", StandardScaler())
])

# Pipeline kategorikal: imputasi 'Unknown' + OneHot
categorical_pipeline = Pipeline(steps=[
  ("imputer", SimpleImputer(strategy="constant", fill_value="Unknown")),
  ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=False))
])

# Gabung semua
preprocess = ColumnTransformer(
  transformers=[
    ("num", numeric_pipeline, numeric_features),
    ("cat", categorical_pipeline, categorical_features),
  ]
)

# Fit transform
X_train_scaled = preprocess.fit_transform(X_train)
X_test_scaled = preprocess.transform(X_test)

st.write("Shape sebelum preprocessing:", X_train.shape)
st.write("Shape sesudah preprocessing:", X_train_scaled.shape)

"""- Pipeline numerik (numeric_pipeline)
  - SimpleImputer(strategy="median") → Isi nilai kosong dengan median (aman terhadap outlier).
  - StandardScaler() → Skala fitur ke rata-rata 0 dan deviasi standar 1 supaya model sensitif skala (mis. SVM, Logistic Regression) bekerja optimal.
- Pipeline kategorikal (categorical_pipeline)
  - SimpleImputer(strategy="constant", fill_value="Unknown") → Isi nilai kosong dengan label "Unknown" agar kategori tetap valid.
  - OneHotEncoder(handle_unknown="ignore") → Ubah kategori menjadi kolom biner, abaikan kategori baru di data uji agar tidak error.
- ColumnTransformer (preprocess) → Menggabungkan kedua pipeline jadi satu proses.
- fit_transform di train, transform di test → Menghindari data leakage, hanya belajar pola dari train.
- Cetak shape → Mengecek berapa banyak fitur baru setelah encoding & scaling.
"""

# Cek mean & std setelah scaling numerik
num_scaled = X_train_scaled[:, :len(numeric_features)]
st.write("\nMean tiap fitur numerik (setelah scaling):", np.round(num_scaled.mean(axis=0), 4))
st.write("Std tiap fitur numerik (setelah scaling):", np.round(num_scaled.std(axis=0), 4))

"""- num_scaled = X_train_scaled[:, :len(numeric_features)] → Mengambil bagian awal array hasil preprocessing yang berisi fitur numerik (karena urutan di ColumnTransformer menaruh numerik dulu).
- mean(axis=0) → Hitung rata-rata setiap fitur numerik setelah scaling.
std(axis=0) → Hitung standar deviasi setiap fitur numerik setelah scaling.
- Tujuan → Mengecek apakah proses StandardScaler() berhasil, yaitu mean ≈ 0 dan std ≈ 1 untuk semua fitur numerik.
- np.round(..., 4) → Membulatkan hasil ke 4 angka di belakang koma supaya lebih rapi dibaca.

#Pemilihan Model
"""

np.random.seed(seed)
random_state=seed

# Load data (fresh) + bersihkan duplikat
URL = "https://raw.githubusercontent.com/hadimaster65555/dataset_for_teaching/refs/heads/main/dataset/bank_churn_dataset_2/Churn_Modelling.csv"
df = pd.read_csv(URL).drop_duplicates().reset_index(drop=True)

# Definisi fitur & target (drop kolom ID/teks)
DROP_COLS = ["Exited", "RowNumber", "CustomerId", "Surname"]
X = df.drop(columns=DROP_COLS, errors="ignore")
y = df["Exited"].astype(int)

# Split stratified (baru SETELAH ini semua transformasi dilakukan)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=test_size, stratify=y, random_state=seed
)

# Sanity check: tidak ada baris identik di train & test
train_hash = pd.util.hash_pandas_object(X_train, index=False)
test_hash  = pd.util.hash_pandas_object(X_test,  index=False)
st.write(f"Overlap baris train–test: {len(set(train_hash).intersection(set(test_hash)))}")

"""- Mengecek apakah ada baris yang identik antara data train dan test, supaya tidak terjadi data leakage.
- pd.util.hash_pandas_object(..., index=False) → Mengubah setiap baris DataFrame menjadi nilai hash unik (seperti sidik jari data).
- set(...).intersection(...) → Mencari hash yang sama di train dan test (artinya baris yang persis sama).
- Output → Jika hasilnya 0, berarti train dan test benar-benar terpisah tanpa duplikasi baris.

Apa yang dilakukan kode dibawah?
- Pisahkan kolom berdasarkan tipe
  - num_cols = ...select_dtypes(...number...) → daftar kolom numerik
  - cat_cols = ...select_dtypes(...object/category/bool...) → daftar kolom kategorikal
  - Kenapa dipisah? Karena perlakuannya beda: angka perlu imputasi median + scaling; kategori perlu imputasi string + one-hot.
- Buat “mini-pipeline” untuk tiap tipe
  - num_pipe:
    - SimpleImputer(strategy="median") → isi nilai hilang angka dengan median (tidak sensitif outlier)
    - StandardScaler() → skala ke mean≈0, std≈1 (bagus buat model seperti logreg/SVM)
  - cat_pipe:
    - SimpleImputer(fill_value="Unknown") → isi kategori yang hilang jadi “Unknown” (jelas dan eksplisit)
    - OneHotEncoder(handle_unknown="ignore", sparse_output=False) → ubah kategori jadi kolom 0/1; kalau ada kategori baru saat test, diabaikan (tidak error)
- Satukan keduanya dengan ColumnTransformer:
  - Artinya: saat transform, kolom numerik diproses oleh num_pipe, kolom kategori oleh cat_pipe, sisanya dibuang (remainder="drop").
"""

# Preprocess di dalam Pipeline (fit HANYA di train)
num_cols = X_train.select_dtypes(include=["int64","float64","int32","float32"]).columns.tolist()
cat_cols = X_train.select_dtypes(include=["object","category","bool"]).columns.tolist()

num_pipe = Pipeline([
  ("imp", SimpleImputer(strategy="median")),
  ("sc", StandardScaler())
])
cat_pipe = Pipeline([
  ("imp", SimpleImputer(strategy="constant", fill_value="Unknown")),
  ("oh", OneHotEncoder(handle_unknown="ignore", sparse_output=False))
])

preprocess = ColumnTransformer([
  ("num", num_pipe, num_cols),
  ("cat", cat_pipe, cat_cols),
], remainder="drop")

"""Kenapa harus di dalam Pipeline & fit HANYA di train?
- Menghindari data leakage.
  - “Fit” itu menghitung statistik dari data: median, mean/std, daftar kategori, dsb.
  - Kalau kamu menghitung itu pakai train + test, informasi test “bocor” ke model → skor jadi kelihatan bagus padahal tidak realistis.
  - Solusinya: fit di X_train saja → preprocess.fit(X_train), lalu apply ke test → preprocess.transform(X_test).
- Konsisten saat cross-validation & production.
  - Dengan Pipeline(preprocess → model), scikit-learn akan otomatis:
    - Fit preprocessing di fold-train tiap CV,
    - Pakai transformasi itu di fold-val,
    - Lalu ulang di fold lain.
    - Ini mencegah kesalahan manual dan menjamin hasil fair.
- Aman dari kategori baru & kolom yang hilang.
  - OneHotEncoder(handle_unknown="ignore") memastikan kalau di test muncul kategori yang tidak ada di train, pipeline tidak meledak; kolomnya diisi nol.
  - Kode rapi & mudah diulang.
  - Semua langkah bersih, satu objek preprocess yang bisa disimpan dan dipakai lagi.
"""

# Model yang akan dibandingkan
models = {
  "Dummy(stratified)": DummyClassifier(strategy="stratified", random_state=seed),
  "LogReg": LogisticRegression(max_iter=1000, class_weight="balanced", random_state=seed),
  "SVM-RBF": SVC(kernel="rbf", C=1.0, gamma="scale", class_weight="balanced", probability=True, random_state=seed),
  "DecisionTree": DecisionTreeClassifier(random_state=seed, class_weight="balanced"),
  "RandomForest": RandomForestClassifier(n_estimators=300, random_state=seed, n_jobs=-1, class_weight="balanced_subsample")}

# Check if XGBoost is available and add it to models if HAS_XGB is True
try:
    from xgboost import XGBClassifier
    HAS_XGB = True
except Exception:
    HAS_XGB = False

if HAS_XGB:
    models["XGBoost"] = XGBClassifier(
        n_estimators=400, max_depth=5, learning_rate=0.05,
        subsample=0.9, colsample_bytree=0.9, reg_lambda=1.0,
        random_state=seed, n_jobs=-1, eval_metric="logloss", tree_method="hist"
    )

"""- Menyiapkan beberapa model yang berbeda untuk dibandingkan performanya dalam memprediksi churn.
- DummyClassifier → Model acak (baseline) untuk pembanding minimal.
- LogisticRegression → Model linear sederhana, cepat, dan interpretatif.
- SVC (SVM-RBF) → Model berbasis kernel untuk menangkap hubungan non-linear.
- DecisionTreeClassifier → supaya mudah diinterpretasi.
- RandomForestClassifier → biasanya akurat dan tahan terhadap overfitting.
- XGBClassifier → Model gradient boosting yang powerful, cepat, dan sering unggul di kompetisi ML.
- class_weight="balanced" / "balanced_subsample" → Mengatasi ketidakseimbangan kelas churn vs non-churn.
- try/except XGBoost → Mengecek apakah XGBoost terpasang; jika tidak, model ini dilewati.
"""

# Train, evaluasi, dan ROC plot
rows = []
plt.figure(figsize=(7,6))
for name, clf in models.items():
  pipe = Pipeline([("prep", preprocess), ("clf", clf)])
  pipe.fit(X_train, y_train)

  # skor probabilitas positif
  if hasattr(pipe.named_steps["clf"], "predict_proba"):
    y_score = pipe.predict_proba(X_test)[:, 1]
  else:
    dec = pipe.decision_function(X_test)
    y_score = (dec - dec.min()) / (dec.max() - dec.min() + 1e-9)

  y_pred = (y_score >= thr).astype(int)

  # metrik
  auc_val = roc_auc_score(y_test, y_score)
  acc = accuracy_score(y_test, y_pred)
  prec = precision_score(y_test, y_pred, zero_division=0)
  rec = recall_score(y_test, y_pred, zero_division=0)
  f1 = f1_score(y_test, y_pred, zero_division=0)
  cm = confusion_matrix(y_test, y_pred)

  rows.append([name, auc_val, acc, prec, rec, f1, cm.tolist()])

  # ROC
  fpr, tpr, _ = roc_curve(y_test, y_score)
  plt.plot(fpr, tpr, lw=2, label=f"{name} (AUC={auc_val:.3f})")

# baseline
fig, ax = plt.subplots(figsize=(6,4))
plt.plot([0,1],[0,1], linestyle="--", lw=1.2, label="Baseline")
plt.title("ROC Curve – Churn (anti-leakage)")
plt.xlabel("False Positive Rate"); plt.ylabel("True Positive Rate")
plt.legend(loc="lower right"); plt.grid(alpha=0.3, linestyle="--"); plt.tight_layout()
st.pyplot(fig)

# Tabel ringkas metrik
result_df = pd.DataFrame(rows, columns=["Model","ROC_AUC","Accuracy","Precision","Recall","F1","ConfusionMatrix"])\
  .sort_values("ROC_AUC", ascending=False)
st.write(result_df.to_string(index=False))

"""#Pelatihan dan Evaluasi Model

Logika umumnya:
  - Kita bandingin semua model (Dummy, Logistic Regression, SVM, Decision Tree, Random Forest, XGBoost)
  - Pakai metrik ROC-AUC karena ini mengukur kemampuan model membedakan kelas positif & negatif di semua threshold (nggak cuma di 0.5).
  - Model dengan AUC tertinggi berarti paling konsisten memisahkan kelas target → di hasilmu kemarin XGBoost dapat 0.858 → ini yang paling unggul.

Makanya, kalau analisis lebih lanjut (Confusion Matrix, ROC Curve detail, Feature Importance), kita pakai pipeline XGBoost yang sama dengan yang menghasilkan 0.858 di tabel.
"""

seed = 42  # satu sumber kebenaran
np.random.seed(seed)

# Asumsi: df sudah ada, target = "Exited"
y = df["Exited"].astype(int)
X = df.drop(columns=["Exited", "RowNumber", "CustomerId", "Surname"], errors="ignore")

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=test_size, stratify=y, random_state=seed
)

# 1) Preprocessor (satu-satunya yang dipakai semua model)
num_cols = X_train.select_dtypes(include=["int64","float64","int32","float32"]).columns.tolist()
cat_cols = X_train.select_dtypes(include=["object","category","bool"]).columns.tolist()

preprocessor = ColumnTransformer(
    transformers=[
        ("num", Pipeline([("imp", SimpleImputer(strategy="median")),
                          ("sc", StandardScaler())]), num_cols),
        ("cat", Pipeline([("imp", SimpleImputer(strategy="constant", fill_value="Unknown")),
                          ("oh", OneHotEncoder(handle_unknown="ignore", sparse_output=False))]), cat_cols),
    ],
    remainder="drop"
)

# 2) Definisikan SEMUA pipeline model (supaya seragam & anti-leakage)
models = (
    "Dummy(stratified)": DummyClassifier(strategy="stratified", random_state=seed,
    "LogReg": LogisticRegression(max_iter=1000, class_weight="balanced", random_state=seed),
    "SVM-RBF": SVC(kernel="rbf", gamma="scale", C=1.0, probability=True, class_weight="balanced", random_state=seed),
    "DecisionTree": DecisionTreeClassifier(random_state=seed, class_weight="balanced"),
    "RandomForest": RandomForestClassifier(
        n_estimators=300, random_state=seed, n_jobs=-1, class_weight="balanced_subsample"
    ),
    "XGBoost": XGBClassifier(
        n_estimators=400, max_depth=5, learning_rate=0.05,
        subsample=0.9, colsample_bytree=0.9,
        reg_lambda=1.0, random_state=seed,
        n_jobs=-1, eval_metric="logloss", tree_method="hist"
    ))

pipelines = {
    name: Pipeline([("prep", preprocessor), ("clf", mdl)]) for name, mdl in models.items()
}

# 3) Train SEMUA pipeline SEKALI, simpan hasil & modelnya
results = []
trained = {}

for name, pipe in pipelines.items():
    pipe.fit(X_train, y_train)
    trained[name] = pipe
    y_proba = pipe.predict_proba(X_test)[:, 1]
    y_pred  = (y_proba >= thr).astype(int)

    auc  = roc_auc_score(y_test, y_proba)
    acc  = (y_pred == y_test).mean()
    prec = ( (y_pred & (y_test==1)).sum() / max((y_pred==1).sum(), 1) )
    rec  = ( (y_pred & (y_test==1)).sum() / max((y_test==1).sum(), 1) )
    f1   = 2*prec*rec / max(prec+rec, 1e-9)

    cm = confusion_matrix(y_test, y_pred).tolist()
    results.append({
        "Model": name, "ROC_AUC": auc, "Accuracy": acc,
        "Precision": prec, "Recall": rec, "F1": f1,
        "ConfusionMatrix": cm
    })

results_df = pd.DataFrame(results).sort_values("ROC_AUC", ascending=False).reset_index(drop=True)
st.write(results_df)

# 4) Pilih model terbaik sesuai tabel (harusnya XGBoost) & PLOT ROC TANPA RETRAIN
best_name = results_df.iloc[0]["Model"]
best_pipe = trained[best_name]

y_score = best_pipe.predict_proba(X_test)[:, 1]
auc_best = roc_auc_score(y_test, y_score)

fpr, tpr, _ = roc_curve(y_test, y_score)
fig, ax = plt.subplots(figsize=(6,4))
plt.plot(fpr, tpr, lw=2, label=f"{best_name} (AUC={auc_best:.3f})")
plt.plot([0,1],[0,1], "--", lw=1.2, label="Baseline", color="orange")
plt.title("ROC Curve – Model Terbaik (tanpa retrain)")
plt.xlabel("False Positive Rate"); plt.ylabel("True Positive Rate")
plt.legend(loc="lower right"); plt.grid(alpha=0.3, linestyle="--"); plt.tight_layout()
st.pyplot(fig)

# 5) (Opsional) Paksa visualisasi khusus XGBoost pakai model yang SAMA
xgb_pipe = trained["XGBoost"]  # ini pipeline persis yang dipakai di tabel
y_score_xgb = xgb_pipe.predict_proba(X_test)[:, 1]
auc_xgb = roc_auc_score(y_test, y_score_xgb)
st.write(f"AUC XGBoost (harus match tabel): {auc_xgb:.6f}")

"""#Hyperparameter Tuning"""

# Load & split (drop duplikat + anti-leakage)
df = pd.read_csv(URL).drop_duplicates().reset_index(drop=True)
y = df["Exited"].astype(int)
X = df.drop(columns=["Exited", "RowNumber", "CustomerId", "Surname"], errors="ignore")

X_train, X_test, y_train, y_test = train_test_split(
  X, y, test_size=test_size, stratify=y, random_state=seed)

# Preprocessing pipeline
num_cols = X_train.select_dtypes(include=["int64","float64","int32","float32"]).columns.tolist()
cat_cols = X_train.select_dtypes(include=["object","category","bool"]).columns.tolist()

num_pipe = Pipeline([("imp", SimpleImputer(strategy="median")),
  ("sc", StandardScaler())])
cat_pipe = Pipeline([("imp", SimpleImputer(strategy="constant", fill_value="Unknown")),
  ("oh", OneHotEncoder(handle_unknown="ignore", sparse_output=False))])

preprocess = ColumnTransformer([
  ("num", num_pipe, num_cols),
  ("cat", cat_pipe, cat_cols),
])

# Definisikan model + ruang hyperparameter
pipe_base = Pipeline([("prep", preprocess), ("clf", LogisticRegression())])  # placeholder, akan diganti di search

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)
scoring = "roc_auc"
n_iter = 30  # naikkan kalau mau lebih teliti

search_spaces = {}

# Logistic Regression (liblinear untuk kestabilan kelas tak seimbang)
search_spaces["LogReg"] = {
  "estimator": LogisticRegression(max_iter=2000, class_weight="balanced", solver="liblinear", random_state=seed),
  "params": {
    "clf__C": np.logspace(-3, 2, 20),
    "clf__penalty": ["l1", "l2"],
  }
}

# SVM-RBF
search_spaces["SVM-RBF"] = {
  "estimator": SVC(kernel="rbf", class_weight="balanced", probability=True, random_state=seed),
  "params": {
    "clf__C": np.logspace(-2, 2, 30),
    "clf__gamma": np.logspace(-4, 1, 30),
  }
}

# Random Forest
search_spaces["RandomForest"] = {
  "estimator": RandomForestClassifier(class_weight="balanced_subsample", random_state=seed, n_jobs=-1),
  "params": {
    "clf__n_estimators": np.arange(200, 701, 50),
    "clf__max_depth": [None] + list(np.arange(3, 21)),
    "clf__min_samples_split": np.arange(2, 21),
    "clf__min_samples_leaf": np.arange(1, 11),
    "clf__max_features": ["sqrt", "log2", 0.5, 0.7, None],
    "clf__bootstrap": [True, False],
  }
}

# XGBoost
if HAS_XGB:
  # Perkirakan scale_pos_weight untuk imbalance
  pos_weight = (len(y_train) - y_train.sum()) / y_train.sum()
  search_spaces["XGBoost"] = {
    "estimator": XGBClassifier(
      objective="binary:logistic", eval_metric="auc", random_state=seed, n_jobs=-1, tree_method="hist",
      scale_pos_weight=pos_weight
    ),
    "params": {
      "clf__n_estimators": np.arange(300, 901, 50),
      "clf__max_depth": np.arange(3, 11),
      "clf__learning_rate": np.logspace(-3, -0.5, 20),
      "clf__subsample": np.linspace(0.6, 1.0, 9),
      "clf__colsample_bytree": np.linspace(0.6, 1.0, 9),
      "clf__reg_lambda": np.logspace(-3, 2, 20),
  }
}

seed = 42
cv_fast = StratifiedKFold(n_splits=2, shuffle=True, random_state=seed)

# Subset train untuk tuning
SUBSET_FRAC = 0.30
X_tune = X_train.sample(frac=SUBSET_FRAC, random_state=seed)
y_tune = y_train.loc[X_tune.index]

search_spaces = {
    "LogReg": {
        "estimator": LogisticRegression(max_iter=2000, class_weight="balanced", solver="liblinear", random_state=seed),
        "params": {
            "clf__C": [0.01, 0.03, 0.1, 0.3, 1, 3, 10],
            "clf__penalty": ["l1", "l2"],
        }
    },
    "RandomForest": {
        "estimator": RandomForestClassifier(class_weight="balanced_subsample",
                                            random_state=seed, n_jobs=-1),
        "params": {
            "clf__n_estimators": [100, 150],
            "clf__max_depth": [None, 8, 12],
            "clf__min_samples_split": [2, 5],
            "clf__min_samples_leaf": [1, 2],
            "clf__max_features": ["sqrt", "log2"],
        }
    }
}

results = []
best_fitted = {}

for name, spec in search_spaces.items():
    st.write(f"\n>>> HalvingRandomSearch (FAST) {name} ...")
    pipe = Pipeline([("prep", preprocess), ("clf", spec["estimator"])])
    hrs = HalvingRandomSearchCV(
        estimator=pipe,
        param_distributions=spec["params"],
        factor=4,
        resource="n_samples",
        min_resources=max(1000, int(0.2*len(X_tune))),
        max_resources=len(X_tune),
        scoring="roc_auc",
        cv=cv_fast,
        random_state=seed,
        n_jobs=1,
        verbose=1
    )
    hrs.fit(X_tune, y_tune)
    results.append({"Model": name, "CV_ROC_AUC": hrs.best_score_, "BestParams": hrs.best_params_})
    best_fitted[name] = hrs.best_estimator_
    st.write(f"Best CV AUC ({name}) = {hrs.best_score_:.3f}")

results_df = pd.DataFrame(results).sort_values("CV_ROC_AUC", ascending=False)
st.write("\n FAST Halving results")
st.write(results_df.to_string(index=False))

# Refit pemenang di FULL training set (bukan subset)
best_name = results_df.iloc[0]["Model"]
best_model = best_fitted[best_name]
best_model.fit(X_train, y_train)

# Evaluasi test
if hasattr(best_model.named_steps["clf"], "predict_proba"):
    y_score = best_model.predict_proba(X_test)[:,1]
else:
    dec = best_model.decision_function(X_test)
    y_score = (dec - dec.min())/(dec.max()-dec.min()+1e-9)
    
y_pred = (y_score >= thr).astype(int)

st.write(f"\n Test metrics ({best_name})")
st.write(f"AUC : {roc_auc_score(y_test, y_score):.3f}")
st.write(f"ACC : {accuracy_score(y_test, y_pred):.3f}")
st.write(f"PRE : {precision_score(y_test, y_pred, zero_division=0):.3f}")
st.write(f"REC : {recall_score(y_test, y_pred, zero_division=0):.3f}")
st.write(f"F1  : {f1_score(y_test, y_pred, zero_division=0):.3f}")

# (opsional) ROC plot
fpr, tpr, _ = roc_curve(y_test, y_score)
fig, ax = plt.subplots(figsize=(6,4))
plt.plot(fpr, tpr, lw=2, label=f"{best_name}")
plt.plot([0,1],[0,1],"--", lw=1.2)
plt.title("ROC – Best Model (FAST Halving)"); plt.xlabel("FPR"); plt.ylabel("TPR")
plt.legend(); plt.grid(alpha=0.3, linestyle="--"); plt.tight_layout(); st.pyplot(fig)

"""Hasil ROC RF lebih baik karena:
  - Plot multi-model yang AUC XGBoost = 0.858 itu kemungkinan dari default parameters atau tuning manual yang udah optimal.
  - Plot Hyperparameter Tuning (FAST Halving) ini hasilnya Random Forest yang keluar sebagai “best model” karena tuning process-nya menemukan kombinasi parameter RF yang performanya lebih tinggi dari parameter XGBoost yang dicoba waktu tuning.

Intinya:
  - Hasil best model tergantung ruang pencarian hyperparameter (param_grid / param_distributions).
  - Kalau parameter XGBoost di tuning kurang luas atau tidak menyentuh kombinasi terbaiknya, hasilnya bisa kalah dari RF yang parameternya pas.
  - Makanya, model terbaik di multi-model test dan hasil tuning bisa berbeda.

#Perbandingan Model
"""

# Load & split
df = pd.read_csv(URL).drop_duplicates().reset_index(drop=True)
y = df["Exited"].astype(int)
X = df.drop(columns=["Exited","RowNumber","CustomerId","Surname"], errors="ignore")

X_train, X_test, y_train, y_test = train_test_split(
  X, y, test_size=test_size, stratify=y, random_state=seed)

# Preprocess (fit hanya di train via pipeline)
num_cols = X_train.select_dtypes(include=["int64","float64","int32","float32"]).columns.tolist()
cat_cols = X_train.select_dtypes(include=["object","category","bool"]).columns.tolist()

num_pipe = Pipeline([("imp", SimpleImputer(strategy="median")),
  ("sc", StandardScaler())])
cat_pipe = Pipeline([("imp", SimpleImputer(strategy="constant", fill_value="Unknown")),
  ("oh", OneHotEncoder(handle_unknown="ignore", sparse_output=False))])

preprocess = ColumnTransformer([
  ("num", num_pipe, num_cols),
  ("cat", cat_pipe, cat_cols),
])

# Kandidat model
models = {
  "LogReg": LogisticRegression(max_iter=1000, class_weight="balanced", random_state=seed),
  # probability=False agar cepat; AUC pakai decision_function
  "SVM-RBF": SVC(kernel="rbf", C=1.0, gamma="scale", class_weight="balanced", probability=False, random_state=seed),
  "DecisionTree": DecisionTreeClassifier(random_state=seed, class_weight="balanced"),
  "RandomForest": RandomForestClassifier(n_estimators=300, random_state=seed, n_jobs=-1, class_weight="balanced_subsample"),
}
if HAS_XGB:
  models["XGBoost"] = XGBClassifier(
    n_estimators=400, max_depth=5, learning_rate=0.05,
    subsample=0.9, colsample_bytree=0.9, reg_lambda=1.0,
    eval_metric="auc", random_state=seed, n_jobs=-1, tree_method="hist")

"""Pemilihan Kandidat Model:
- Model dipilih untuk mewakili beragam pendekatan algoritmik pada data tabular, sehingga evaluasi mencakup metode linier, non-linier, pohon keputusan, dan ensemble.
  - Logistic Regression: Benchmark linier dengan interpretabilitas tinggi.
  - SVM-RBF: Menangkap pola non-linier tanpa transformasi eksplisit.
  - Decision Tree: Interpretasi jelas dan mampu memodelkan interaksi fitur.
  - Random Forest: Ensemble pohon keputusan yang stabil dan tahan terhadap varians.
  - XGBoost: Gradient boosting teroptimasi dengan performa unggul pada data kompleks.

- Model terbaik ditentukan berdasarkan skor AUC tertinggi dari hasil evaluasi.
"""

# Evaluasi: CV (ROC-AUC) + Test metrics
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)
rows, fitted = [], {}

for name, clf in models.items():
  pipe = Pipeline([("prep", preprocess), ("clf", clf)])

  # CV ROC-AUC di training
  cv_auc = cross_val_score(pipe, X_train, y_train, cv=cv, scoring="roc_auc", n_jobs=1)
  pipe.fit(X_train, y_train)

  # Skor probabilitas untuk test AUC
  if hasattr(pipe.named_steps["clf"], "predict_proba"):
    y_score = pipe.predict_proba(X_test)[:, 1]
  else:
    dec = pipe.decision_function(X_test)
    # normalisasi ke [0,1] agar kompatibel untuk AUC (opsional)
    y_score = (dec - dec.min()) / (dec.max() - dec.min() + 1e-9)

  y_pred = (y_score >= thr).astype(int)

  rows.append({
    "Model": name,
    "CV_ROC_AUC_mean": cv_auc.mean(),
    "CV_ROC_AUC_std":  cv_auc.std(),
    "Test_ROC_AUC":    roc_auc_score(y_test, y_score),
    "Test_Accuracy":   accuracy_score(y_test, y_pred),
    "Test_Precision":  precision_score(y_test, y_pred, zero_division=0),
    "Test_Recall":     recall_score(y_test, y_pred, zero_division=0),
    "Test_F1":         f1_score(y_test, y_pred, zero_division=0),
  })
  fitted[name] = pipe

results_df = pd.DataFrame(rows).sort_values("Test_ROC_AUC", ascending=False)
st.write("\n Perbandingan Model (urut AUC test)")
pd.options.display.float_format = "{:.3f}".format
st.write(results_df.to_string(index=False))

best_name = results_df.iloc[0]["Model"]
st.write(f"\nModel terbaik berdasarkan Test ROC-AUC: {best_name}")

"""Pemilihan Model Terbaik:
Dari hasil evaluasi, XGBoost memperoleh skor AUC tertinggi dibanding kandidat lain, menunjukkan kemampuan terbaik dalam membedakan kelas positif dan negatif pada data ini.
Selain itu, XGBoost menggabungkan kekuatan boosting dengan pengaturan regulasi yang fleksibel, sehingga mampu menangani fitur numerik maupun kategorikal, mengurangi risiko overfitting, dan mempertahankan kinerja tinggi pada data dengan distribusi yang kompleks.
"""

# Plot ROC untuk 3 model teratas
top_k = results_df["Model"].head(3).tolist()
fig, ax = plt.subplots(figsize=(7,4))
for name in top_k:
  pipe = fitted[name]
  if hasattr(pipe.named_steps["clf"], "predict_proba"):
    y_score = pipe.predict_proba(X_test)[:, 1]
  else:
    dec = pipe.decision_function(X_test)
    y_score = (dec - dec.min()) / (dec.max() - dec.min() + 1e-9)
  fpr, tpr, _ = roc_curve(y_test, y_score)
  auc_val = roc_auc_score(y_test, y_score)
  plt.plot(fpr, tpr, lw=2, label=f"{name} (AUC={auc_val:.3f})")

plt.plot([0,1],[0,1], "--", lw=1.2, label="Baseline")
plt.title("ROC Curve – Top 3 Model")
plt.xlabel("False Positive Rate"); plt.ylabel("True Positive Rate")
plt.legend(loc="lower right"); plt.grid(alpha=0.3, linestyle="--"); plt.tight_layout()
st.pyplot(fig)

"""#Interpretasi dan Wawasan

Alasan kenapa perlu interpretasi mendalam (kita buat sampai visualisasi DALEX dan ICE)
1. Transparansi model
  - Tanpa interpretasi, model jadi “black box” → kita tahu hasil prediksi, tapi tidak tahu kenapa.
  - Interpretasi membantu membangun kepercayaan tim bisnis, stakeholder, atau regulator.

2. Insight untuk strategi bisnis
  - Prediksi saja hanya bilang “si A kemungkinan churn”.
  - Interpretasi menjawab “karena apa si A churn?” dan “apa pola umumnya di seluruh pelanggan?”.
  - Ini berguna untuk membuat kampanye yang tepat sasaran.

3. Validasi model
  - ICE/PDP/DALEX bisa mengungkap jika model belajar pola yang tidak masuk akal atau bias dari data.
  - Kalau tidak dilakukan, kita bisa saja pakai model yang secara metrik bagus, tapi salah secara logika.

4. Prioritas intervensi
  - Dengan mengetahui fitur paling berpengaruh, perusahaan bisa fokus ke variabel yang bisa diubah atau dikontrol.
  - Misalnya, jika Balance tinggi ternyata terkait churn, tim bisa bikin program retensi khusus untuk segmen itu.

Kalau tidak pakai interpretasi (DALEX, ICE, dll.)
  - Kita tetap bisa punya model yang memprediksi churn dengan baik (AUC tinggi, F1 bagus (berdasarkan perbandingan model, pelatihan dan evaluasi model, hingga hyperparameter tuning)).
  - Tapi kita tidak tahu alasan di balik prediksi → sulit untuk membuat kebijakan yang tepat.
  - Risiko membuat intervensi tidak efektif atau bahkan salah sasaran meningkat.
  - Dalam konteks bisnis nyata, ini berarti kehilangan peluang untuk menghemat biaya atau meningkatkan ROI kampanye.
"""

df = pd.read_csv(URL).drop_duplicates().reset_index(drop=True)
y = df["Exited"].astype(int)
X = df.drop(columns=["Exited","RowNumber","CustomerId","Surname"], errors="ignore")

X_train, X_test, y_train, y_test = train_test_split(
  X, y, test_size=test_size, stratify=y, random_state=seed
)

num_cols = X_train.select_dtypes(include=["int64","float64","int32","float32"]).columns.tolist()
cat_cols = X_train.select_dtypes(include=["object","category","bool"]).columns.tolist()

num_pipe = Pipeline([("imp", SimpleImputer(strategy="median")), ("sc", StandardScaler())])
cat_pipe = Pipeline([("imp", SimpleImputer(strategy="constant", fill_value="Unknown")), ("oh", OneHotEncoder(handle_unknown="ignore", sparse_output=False))])

preprocess = ColumnTransformer([
  ("num", num_pipe, num_cols),
  ("cat", cat_pipe, cat_cols),
])

pipe = Pipeline([
  ("prep", preprocess),
  ("clf", RandomForestClassifier(
    n_estimators=300, random_state=seed, n_jobs=-1,
    class_weight="balanced_subsample"
  ))
])

pipe.fit(X_train, y_train)

# Ringkasan metrik + Confusion Matrix
if hasattr(pipe.named_steps["clf"], "predict_proba"):
  y_score = pipe.predict_proba(X_test)[:,1]
else:
  dec = pipe.decision_function(X_test)
  y_score = (dec - dec.min())/(dec.max()-dec.min()+1e-9)
y_pred = (y_score >= thr).astype(int)

st.write("\n Evaluasi Test (threshold=0.5)")
st.write(f"ROC-AUC   : {roc_auc_score(y_test, y_score):.3f}")
st.write(f"Accuracy  : {accuracy_score(y_test, y_pred):.3f}")
st.write(f"Precision : {precision_score(y_test, y_pred, zero_division=0):.3f}")
st.write(f"Recall    : {recall_score(y_test, y_pred, zero_division=0):.3f}")
st.write(f"F1-Score  : {f1_score(y_test, y_pred, zero_division=0):.3f}")
st.write("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))
st.write("\nClassification Report:\n", classification_report(y_test, y_pred, zero_division=0))

# Hitung confusion matrix
cm = confusion_matrix(y_test, y_pred)

fig, ax = plt.subplots(figsize=(6,4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
  xticklabels=["No Churn", "Churn"],
  yticklabels=["No Churn", "Churn"])
plt.ylabel('True label')
plt.xlabel('Predicted label')
plt.title("Confusion Matrix - Model Evaluasi")
st.pyplot(fig)

try:
  y_score
except NameError:
  # Ganti 'pipe' dan 'X_test' sesuai variabelmu
  if hasattr(pipe.named_steps["clf"], "predict_proba"):
    y_score = pipe.predict_proba(X_test)[:, 1]
  else:
    dec = pipe.decision_function(X_test)
    y_score = (dec - dec.min()) / (dec.max() - dec.min() + 1e-9)

# ROC Curve + Youden point
fpr, tpr, thr = roc_curve(y_test, y_score)
auc_val = roc_auc_score(y_test, y_score)

# Youden's J statistic: tpr - fpr
youden_idx = np.argmax(tpr - fpr)
thr_youden = float(thr[youden_idx])

fig, ax = plt.subplots(figsize=(6.2, 5.2))
plt.plot(fpr, tpr, lw=2, label=f"Model (AUC={auc_val:.3f})")
plt.plot([0, 1], [0, 1], "--", lw=1.2, label="Baseline")
plt.scatter(fpr[youden_idx], tpr[youden_idx], s=60, edgecolor="k", label=f"Youden* (thr≈{thr_youden:.2f})")
plt.title("ROC Curve")
plt.xlabel("False Positive Rate"); plt.ylabel("True Positive Rate")
plt.legend(loc="lower right"); plt.grid(alpha=0.3, linestyle="--")
plt.tight_layout(); st.pyplot(fig)

"""1. ROC Curve & AUC
- ROC Curve menunjukkan trade-off antara True Positive Rate (TPR) dan False Positive Rate (FPR) pada berbagai threshold.
- AUC (Area Under Curve) adalah luas area di bawah ROC curve. Nilai AUC yang semakin mendekati 1 berarti model makin baik dalam membedakan kelas positif dan negatif.

2. Youden’s J Statistic
- Rumus: J = TPR – FPR
- Mencari threshold yang memaksimalkan nilai J berarti mencari titik optimal yang menyeimbangkan sensitivitas (TPR) dan spesifisitas (1 - FPR).
- Pada kode ini:
  - youden_idx adalah indeks titik optimal.
  - thr_youden adalah nilai probabilitas (threshold) pada titik optimal itu.

3. Interpretasi Visual
- Garis biru = ROC curve dari model.
- Garis putus-putus = baseline (model acak).
- Titik hitam (scatter) = titik Youden, yaitu threshold terbaik berdasarkan keseimbangan TPR dan FPR.
- Label pada titik menunjukkan nilai threshold, sehingga kita bisa tahu di probabilitas berapa model sebaiknya memutuskan prediksi positif agar performa optimal.

Youden Point adalah titik pada ROC Curve yang memaksimalkan selisih True Positive Rate (TPR) dan False Positive Rate (FPR).
  -  Titik ini dipilih karena memberikan threshold optimal untuk memisahkan kelas positif dan negatif dengan keseimbangan terbaik antara sensitivitas (recall) dan spesifisitas.
  - Pada grafik ROC (gambar pertama), titik ini ditandai sebagai lingkaran hitam.
  - Threshold dari Youden Point ini kemudian digunakan untuk menganalisis trade-off Precision, Recall, dan F1 (gambar kedua).
  
Singkatnya: Model dilatih → diuji pada data test → kinerja diukur dengan metrik umum → threshold optimal dipilih menggunakan Youden Point untuk hasil prediksi yang seimbang.
"""

# Trade-off Precision / Recall / F1 vs Threshold
grid = np.linspace(0.01, 0.99, 99)
prec_list, rec_list, f1_list = [], [], []
for t in grid:
  yp = (y_score >= t).astype(int)
  prec_list.append(precision_score(y_test, yp, zero_division=0))
  rec_list.append(recall_score(y_test, yp, zero_division=0))
  f1_list.append(f1_score(y_test, yp, zero_division=0))

fig, ax = plt.subplots(figsize=(7.2, 4.6))
plt.plot(grid, prec_list, label="Precision")
plt.plot(grid, rec_list,  label="Recall")
plt.plot(grid, f1_list,   label="F1")
plt.axvline(thr_youden, ls="--", color="gray", label=f"Youden* ({thr_youden:.2f})")
plt.title("Precision / Recall / F1 vs Threshold")
plt.xlabel("Threshold"); plt.ylabel("Score"); plt.legend()
plt.grid(alpha=0.3, linestyle="--"); plt.tight_layout(); st.pyplot(fig)

"""1. Kenapa tidak pakai threshold default 0.5 saja?
  - Threshold 0.5 itu arbitrary (default di banyak library), bukan threshold optimal untuk semua kasus.
  - Kalau data tidak seimbang (misalnya churn jauh lebih sedikit dari non-churn, seperti data kita), threshold 0.5 sering menghasilkan recall rendah → banyak churn yang tidak terdeteksi.
  - Dalam kasus bisnis seperti churn, false negative (FN) itu mahal, karena berarti pelanggan yang akan pergi tidak diantisipasi.

2. Kenapa pakai Youden Point
  - Youden Point adalah titik di ROC curve yang memaksimalkan nilai:
Youden’s J=TPR−FPR

3. Artinya, ini adalah titik di mana sensitivitas (recall) dan spesifisitas berada dalam kombinasi terbaik.
  - TPR (Recall) tinggi → churn lebih banyak terdeteksi.
  - FPR rendah → meminimalkan salah deteksi.

Hasilnya, kita dapat threshold yang balance antara mendeteksi positif dan menghindari false alarm.

1. Kenapa Youden Point tidak di tengah?
  - Titik Youden (J = TPR – FPR) ditentukan dari ROC curve, bukan dari grafik Precision–Recall–F1 vs Threshold.
  - Jadi, 0.24 di sini adalah threshold yang secara matematis memaksimalkan keseimbangan antara sensitivitas (Recall) dan spesifisitas (1 – FPR), walaupun dari sisi F1 atau Precision–Recall posisinya bukan di tengah.

2. Kenapa terlihat condong ke kiri?
  - Pada data ini, model cenderung menghasilkan probabilitas yang tidak simetris untuk kelas positif & negatif.
  - Akibatnya, threshold optimal dari sisi ROC bisa berada di area rendah (misal 0.24), supaya Recall cukup tinggi tanpa mengorbankan terlalu banyak spesifisitas.
  - Kalau threshold diletakkan di 0.5 (tengah), kemungkinan Recall turun banyak dan FPR juga berubah, sehingga J statistic malah lebih kecil.

3. Implikasinya
  - Threshold 0.24 ini bukan yang memaksimalkan F1 (hijau), tapi yang memaksimalkan J (dari ROC).
  - Kalau tujuan proyek adalah menyeimbangkan Sensitivitas dan Spesifisitas, maka pakai 0.24.
  - Kalau tujuannya memaksimalkan F1 Score, titiknya mungkin agak geser dari 0.24.

Threshold optimal dari Youden Point (~0.24), perlu dilakukan evaluasi ulang kinerja model pada threshold tersebut, lalu dibandingkan dengan threshold default (0.50).

Alasannya:
1. Mengetahui Dampak Threshold terhadap Kinerja
  - Threshold default 0.50 biasanya digunakan pada banyak model, tapi belum tentu memberikan keseimbangan terbaik antara recall dan specificity.
  - Dengan threshold Youden (0.24), model lebih sensitif terhadap kasus Churn, sehingga recall meningkat (0.727 vs 0.442), walaupun presisi sedikit turun.

2. Analisis Perubahan Confusion Matrix
  - Threshold 0.50 → lebih sedikit positif terprediksi, recall rendah → banyak churn yang terlewat.
  - Threshold 0.24 → lebih banyak positif terprediksi, recall naik → lebih banyak churn terdeteksi, walaupun ada peningkatan false positive.

3. Menyesuaikan dengan Tujuan Bisnis
  - Pada kasus churn prediction, biasanya lebih penting mengurangi false negative (tidak mendeteksi pelanggan yang akan churn) dibanding menjaga presisi tinggi. Itu sebabnya, setelah threshold optimal ditentukan, dibuat confusion matrix untuk melihat secara jelas jumlah TP, TN, FP, dan FN pada threshold tersebut.
"""

# Metrik & Confusion Matrix (0.5 vs Youden)
def metrics_at_threshold(y_true, y_prob, thr_val):
  yp = (y_prob >= thr_val).astype(int)
  return {
    "thr": thr_val,
    "ACC": accuracy_score(y_true, yp),
    "PRE": precision_score(y_true, yp, zero_division=0),
    "REC": recall_score(y_true, yp, zero_division=0),
    "F1" : f1_score(y_true, yp, zero_division=0),
    "CM" : confusion_matrix(y_true, yp)
  }

res_05 = metrics_at_threshold(y_test, y_score, 0.50)
res_y  = metrics_at_threshold(y_test, y_score, thr_youden)

st.write("\n Metrics @ threshold 0.50")
st.write(f"ACC={res_05['ACC']:.3f}  PRE={res_05['PRE']:.3f}  REC={res_05['REC']:.3f}  F1={res_05['F1']:.3f}")
st.write("Confusion Matrix:\n", res_05["CM"])

st.write(f"\n Metrics @ threshold Youden* ({thr_youden:.3f})")
st.write(f"ACC={res_y['ACC']:.3f}  PRE={res_y['PRE']:.3f}  REC={res_y['REC']:.3f}  F1={res_y['F1']:.3f}")
st.write("Confusion Matrix:\n", res_y["CM"])

# Heatmap untuk threshold Youden
labels = ["No Churn", "Churn"]
fig, ax = plt.subplots(figsize=(5.3, 4.6))
sns.heatmap(res_y["CM"], annot=True, fmt="d", cmap="Blues", cbar=False, xticklabels=labels, yticklabels=labels)
plt.xlabel("Prediksi"); plt.ylabel("Aktual")
plt.title(f"Confusion Matrix (thr≈{thr_youden:.2f})")
plt.tight_layout(); st.pyplot(fig)

"""Ini dilanjutkan ke Permutation Importance supaya tahu fitur mana yang paling berpengaruh ke prediksi model.
Dilakukan setelah OHE agar setiap kategori fitur bisa diukur pengaruhnya.
Hasilnya menunjukkan fitur seperti Age, NumOfProducts, dan Balance paling berpengaruh terhadap AUC model.
"""

from sklearn.inspection import permutation_importance
import numpy as np, pandas as pd, matplotlib.pyplot as plt

prep = pipe.named_steps["prep"]
clf  = pipe.named_steps["clf"]

# ruang fitur setelah transform
X_test_t = prep.transform(X_test)

# nama fitur setelah OHE
ohe = prep.named_transformers_["cat"].named_steps["oh"]
num_cols = prep.transformers_[0][2]
cat_cols = prep.transformers_[1][2]
feat_names_out = list(num_cols) + list(ohe.get_feature_names_out(cat_cols))

r = permutation_importance(
  clf, X_test_t, y_test,
  n_repeats=10,
  random_state=42,
  scoring="roc_auc",
  n_jobs=1
)

imp = (pd.DataFrame({"feature": feat_names_out, "importance": r.importances_mean})
  .sort_values("importance", ascending=False)
  .head(20))

st.write(imp.to_string(index=False))

fig, ax = plt.subplots(figsize=(8,6))
plt.barh(imp["feature"][::-1], imp["importance"][::-1])
plt.title("Permutation Importance (fitur setelah OHE, AUC)")
plt.xlabel("Mean importance")
plt.tight_layout(); st.pyplot(fig)

"""Untuk memahami hubungan antara fitur penting dan prediksi model secara lebih mendalam. Alasannya:
  - Menjelaskan model – Setelah tahu fitur penting dari Permutation Importance, PDP/ICE menjawab “bagaimana perubahan nilai fitur memengaruhi probabilitas churn.”
  - Insight bisnis – Misalnya di plot Age, bisa terlihat bahwa risiko churn naik di usia tertentu, yang bisa jadi dasar strategi retensi.
  - Model interpretability – Dalam banyak kasus, terutama jika model akan dipresentasikan ke manajemen atau regulator, kita butuh bukti kenapa model memutuskan demikian.
  - Validasi model – Mengecek apakah pola yang diambil model masuk akal atau justru aneh (misalnya menemukan tren yang tidak logis, yang bisa menandakan data bermasalah).

Singkatnya:
PDP/ICE tidak penting untuk menjelaskan keputusan model dan mendapatkan insight actionable.
"""

# Partial Dependence (PDP/ICE) untuk fitur kunci

# pilih beberapa fitur numerik asli (bukan hasil OHE)
focus_feats = [c for c in ["Age","CreditScore","Balance","NumOfProducts","Tenure"] if c in X.columns]
if focus_feats:
  for f in focus_feats:
    fig, ax = plt.figure(figsize=(5.5,4.2))
    PartialDependenceDisplay.from_estimator(pipe, X, [f], kind="both", grid_resolution=30)
    plt.title(f"PDP/ICE – {f}")
    plt.tight_layout(); st.pyplot(fig)

"""Mengulang analisis feature importance (Permutation Importance) tapi kali ini langsung menggunakan X_test tanpa memperlihatkan detail OHE, lalu divisualisasikan dalam bentuk bar chart Top-20 supaya:
  - Lebih ringkas dan fokus – langsung melihat fitur yang paling berpengaruh pada model di data uji.
  - Memudahkan interpretasi – urutan fitur dan besar pengaruhnya terlihat jelas di grafik.
  - Konsistensi hasil – bisa dicek apakah urutan pentingnya fitur sama dengan perhitungan sebelumnya (validasi temuan).

Singkatnya, ini adalah tahap interpretasi akhir untuk menegaskan fitur kunci model setelah semua evaluasi performa selesai.
"""

# Hitung permutation importance
r = permutation_importance(pipe, X_test, y_test,
  n_repeats=5,
  random_state=42,
  scoring='roc_auc')

# Urutkan dan ambil 20 teratas
feat_names = X.columns
imp_df = pd.DataFrame({
  "Feature": feat_names,
  "Importance": r.importances_mean
}).sort_values("Importance", ascending=False).head(20)

# Plot bar chart
fig, ax = plt.subplots(figsize=(8,5))
plt.barh(imp_df["Feature"], imp_df["Importance"])
plt.gca().invert_yaxis()
plt.xlabel("Mean Importance (AUC drop)")
plt.title("Top-20 Feature Importance (Permutation)")
st.pyplot(fig)

"""Kita melakukan ini karena Lift Curve dan Cumulative Gain Curve adalah alat evaluasi yang sangat berguna di konteks bisnis seperti churn prediction:
1. Mengukur efektivitas model dibanding random targeting
  - Cumulative Gain: menunjukkan seberapa besar proporsi churn yang bisa terdeteksi jika kita hanya menargetkan persentase tertentu pelanggan dengan skor prediksi tertinggi.
  - Lift: menunjukkan “peningkatan” (multiplikasi) efektivitas dibanding memilih pelanggan secara acak.
2. Output siap untuk keputusan bisnis
  - Kode ini menghitung tabel per-decile → memudahkan manajer marketing melihat “Kalau saya target 20% pelanggan teratas, saya bisa menangkap berapa % churn?”
  - Hal ini langsung actionable untuk alokasi sumber daya (misalnya promosi, penawaran khusus).
3. Fleksibel untuk berbagai model
  - Fungsi get_positive_scores bisa menangani model yang output-nya berupa predict_proba atau decision_function.
"""

# Lift & Cumulative Gain (butuh scikit-plot)

# Helper ambil skor positif dari pipeline/model apa pun
def get_positive_scores(estimator, X):
  "Skor probabilitas kelas positif (churn=1) untuk berbagai model."
  if hasattr(estimator, "predict_proba"):
    proba = estimator.predict_proba(X)
    if proba.ndim == 2:
      if hasattr(estimator, "classes_") and 1 in estimator.classes_:
        idx = list(estimator.classes_).index(1)
        return proba[:, idx]
      return proba[:, -1]
    return proba.ravel()
  if hasattr(estimator, "decision_function"):
    dec = np.asarray(estimator.decision_function(X)).ravel()
    return (dec - dec.min()) / (dec.max() - dec.min() + 1e-12)
  # fallback kasar:
  return estimator.predict(X).astype(float)

# Hitung skor & siapkan y sebagai array 0/1
scores = get_positive_scores(pipe, X_test)
y_true = np.asarray(y_test).astype(int)
n = len(y_true)
pos = y_true.sum()

# Urutkan berdasarkan skor (desc) dan hitung curve
order = np.argsort(scores)[::-1]
y_sorted = y_true[order]

cum_positives = np.cumsum(y_sorted)
perc_samples  = np.arange(1, n+1) / n
cumu_gain     = cum_positives / pos
lift          = cumu_gain / perc_samples

# Plot Cumulative Gain
fig, ax = plt.subplots(figsize=(6.6, 5.0))
plt.plot(perc_samples, cumu_gain, lw=2, label="Model")
plt.plot([0,1], [0,1], "--", lw=1.2, label="Baseline (acak)")
plt.title("Cumulative Gain")
plt.xlabel("Proporsi Sampel yang Ditarget")
plt.ylabel("Proporsi Churn Terdeteksi")
plt.legend(loc="lower right")
plt.grid(alpha=0.3, linestyle="--")
plt.tight_layout()
st.pyplot(fig)

# Plot Lift
fig, ax = plt.subplots(figsize=(6.6, 5.0))
plt.plot(perc_samples, lift, lw=2, label="Model")
plt.plot([0,1], [1,1], "--", lw=1.2, label="Baseline (Lift=1)")
plt.title("Lift Curve")
plt.xlabel("Proporsi Sampel yang Ditarget")
plt.ylabel("Lift")
plt.legend(loc="upper right")
plt.grid(alpha=0.3, linestyle="--")
plt.tight_layout()
st.pyplot(fig)

# Tabel per-decile untuk laporan bisnis
# Menunjukkan berapa % churn tertangkap jika menargetkan top 10%, 20%, ... dari skor
deciles = np.linspace(0.1, 1.0, 10)
rows = []
for d in deciles:
  k = int(np.ceil(d * n))
  caught = y_sorted[:k].sum()
  gain = caught / pos
  rows.append([int(d*100), k, caught, gain])
    
lift_table = pd.DataFrame(rows, columns=["% Sampel", "N Ditarget", "Churn Tertangkap", "Cumulative Gain"])
st.write("\n Tabel Gain per-Decile")
st.write(lift_table.to_string(index=False, formatters={"Cumulative Gain": "{:.3f}".format}))

"""Interpretasinya jadi seperti ini:
1. Cumulative Gain
  - Garis biru (model) menunjukkan peningkatan deteksi churn yang jauh di atas garis baseline acak (oranye).
  - Misalnya:
    - 20% pelanggan teratas → berhasil mendeteksi 60,2% churn (245 pelanggan churn dari 407 total churn).
    - 30% pelanggan teratas → menangkap 74% churn.
    - 50% pelanggan teratas → sudah menangkap 87,5% churn.
  - Artinya, model sangat efektif untuk mengidentifikasi pelanggan yang paling berisiko churn di kelompok atas.

2. Lift Curve
  - 10% pelanggan teratas memiliki lift ≈ 5 → kemungkinan menemukan churn di kelompok ini 5× lebih besar dibanding pemilihan acak.
  - Lift menurun secara bertahap seiring bertambahnya proporsi target, dan mendekati 1 ketika semua pelanggan ditarget (yang berarti sama dengan acak).
3. Dampak Bisnis
  - Jika hanya menargetkan 20% pelanggan dengan skor churn tertinggi, perusahaan sudah bisa menangkap lebih dari 60% kasus churn, sehingga biaya kampanye dapat ditekan tanpa kehilangan sebagian besar pelanggan berisiko.
  - Bahkan pada 10% pelanggan teratas, sudah bisa menangkap hampir 40% churn, artinya upaya intervensi bisa sangat fokus pada kelompok yang paling kritis.
  - Strategi ini memberikan ROI tinggi, karena sumber daya marketing dan retensi diarahkan ke segmen yang memberi dampak terbesar terhadap pengurangan churn.

Dilanjutkan ke DALEX karena tujuannya adalah melakukan interpretasi global dan lokal model — untuk memahami mengapa model membuat prediksi tertentu, bukan hanya seberapa bagus performanya. Kenapa perlu DALEX
  - Mengetahui kontribusi setiap fitur terhadap kinerja model (Variable Importance).
  - Melihat hubungan pola antara nilai fitur dan probabilitas churn (Partial Dependence / Variable Profile).
  - Memberikan transparansi → penting untuk menjelaskan model ke tim bisnis atau regulator, sehingga tidak hanya jadi “black box”.
"""

# Commented out IPython magic to ensure Python compatibility.
# Interpretasi global dengan DALEX
# kalau belum terpasang
# %pip install -q dalex

# Check if DALEX is available
try:
    import dalex as dx
    HAS_DALEX = True
except ImportError:
    HAS_DALEX = False
    st.write("DALEX not installed. Skipping DALEX interpretation.")


if HAS_DALEX and hasattr(pipe.named_steps["clf"], "predict_proba"):
  explainer = dx.Explainer(
    model=pipe, data=X_test, y=y_test,
    predict_function=lambda m, X_: m.predict_proba(X_)[:,1],
    label="ChurnModel"
  )
  vi = explainer.model_parts(loss_function="auc")  # kontribusi variabel ke AUC
  st.write("\nDALEX – Variable Importance (head):")
  st.write(vi.result.head(10))

  vp = explainer.model_profile(variables=focus_feats or num_cols[:3])

  # DALEX plot
  if vp:
    st.write("\nDALEX – Variable Profile (Partial Dependence) plots:")
    fig = vp.plot()
    st.pyplot(fig)

"""Interpretasi Grafik
1. Variable Importance (dropout loss)
  - Age memiliki pengaruh terbesar terhadap AUC model (nilai tertinggi), diikuti oleh Balance, Geography, dan NumOfProducts.
  - Fitur seperti Tenure dan EstimatedSalary memiliki pengaruh kecil terhadap prediksi churn.

2. Partial Dependence / Variable Profiles
  - Age: Probabilitas churn meningkat signifikan mulai umur sekitar 40, puncak di sekitar 50–60, lalu menurun.
  - CreditScore: Semakin tinggi skor kredit, probabilitas churn sedikit menurun (efek tidak besar).
  - Balance: Pelanggan dengan saldo sangat tinggi cenderung sedikit lebih mungkin churn.
  - NumOfProducts: Pelanggan dengan 3–4 produk memiliki probabilitas churn lebih tinggi dibanding yang hanya punya 1–2 produk.
  - Tenure: Hampir tidak ada pengaruh signifikan, menunjukkan lama menjadi nasabah bukan faktor dominan di sini.
"""

if 'imp' in locals():
  st.write("\nTop Features and their Importance:")
  for index, row in imp.iterrows():
    st.write(f"- {row['feature']} memiliki pengaruh {row['importance']:.3f} terhadap churn")
else:
  st.write("Feature importance results ('imp' DataFrame) not found. Please run the permutation importance cell first.")

"""#Insight Actionable untuk Bank
1. Segmentasi & Retensi Saldo Rendah
  - Identifikasi nasabah dengan saldo rata-rata < X juta.
  - Kirim promo menabung atau bunga tinggi untuk deposito kecil.

2. Program Aktivasi Nasabah Pasif
  - Kirim notifikasi personal untuk ajakan transaksi.
  - Tawarkan cashback atau poin reward untuk transaksi dalam 30 hari ke depan.

3. Fokus Wilayah Churn Tinggi
  - Audit kualitas pelayanan cabang tersebut.
  - Adakan program community engagement (misalnya edukasi keuangan).

4. Bundling Produk untuk Nasabah Potensial Churn
  - Gabungkan rekening tabungan dengan kartu kredit atau pinjaman mikro.

5. Paket Khusus Generasi Muda
  - Mobile banking dengan gamifikasi dan promo lifestyle (misalnya diskon makan/minum).
"""

# IDENTIFIKASI FAKTOR KUNCI

try:
  from xgboost import XGBClassifier
  HAS_XGB = True
except Exception:
  HAS_XGB = False

# ambil komponen pipeline & nama fitur setelah transform
prep = pipe.named_steps["prep"]
clf  = pipe.named_steps["clf"]

X_test_t = prep.transform(X_test)

ohe = prep.named_transformers_["cat"].named_steps["oh"]
num_cols = prep.transformers_[0][2]
cat_cols = prep.transformers_[1][2]
feat_names_ohe = list(num_cols) + list(ohe.get_feature_names_out(cat_cols))

# ambil importance ringan sesuai tipe model
if isinstance(clf, RandomForestClassifier) or (HAS_XGB and isinstance(clf, XGBClassifier)):
  imp_vec = getattr(clf, "feature_importances_", None)
elif isinstance(clf, LogisticRegression):
  imp_vec = np.abs(clf.coef_[0])
else:
  from sklearn.inspection import permutation_importance
  rng = np.random.RandomState(42)
  idx = rng.choice(X_test_t.shape[0], size=min(400, X_test_t.shape[0]), replace=False)
  r = permutation_importance(clf, X_test_t[idx], y_test.values[idx],
    n_repeats=2, random_state=42, scoring="roc_auc", n_jobs=1)
  imp_vec = r.importances_mean

# safety
imp_vec = np.asarray(imp_vec).ravel()
assert len(imp_vec) == X_test_t.shape[1], (len(imp_vec), X_test_t.shape[1])

imp_ohe_df = (pd.DataFrame({"Feature_OHE": feat_names_ohe, "Importance": imp_vec})
  .sort_values("Importance", ascending=False)
  .reset_index(drop=True))

# agregasi OHE -> fitur mentah (ringkas untuk non-teknis)
def to_base_name(s):
  if s in num_cols:
    return s
  for c in cat_cols:
    pref = f"{c}_"
    if s.startswith(pref):
      return c
  return s

imp_group = (imp_ohe_df.assign(Base=imp_ohe_df["Feature_OHE"].apply(to_base_name))
  .groupby("Base", as_index=False)["Importance"].sum()
  .sort_values("Importance", ascending=False)
  .reset_index(drop=True))

st.write("\n TOP 10 FAKTOR KUNCI (agregasi per fitur mentah)")
st.write(imp_group.head(10).to_string(index=False, formatters={"Importance":"{:.4f}".format}))

st.write("\n TOP 10 DETAIL KATEGORI (setelah OHE)")
st.write(imp_ohe_df.head(10).to_string(index=False, formatters={"Importance":"{:.4f}".format}))

# rekomendasi aksi ringan berbasis nama fitur (tanpa komputasi berat)
def recommend(feat):
  f = feat.lower()
  if "balance" in f:
    return "Program peningkatan saldo: bonus bunga, auto-save, bundling deposito kecil."
  if "isactivemember" in f:
    return "Kampanye aktivasi: cashback transaksi pertama, reminder rutin, tutorial app."
  if "numofproducts" in f:
    return "Cross-sell: kartu kredit, e-wallet/auto-debit, tabungan berjangka."
  if "age" in f:
    return "Paket usia: Gen-Z/milenial (promo lifestyle), usia tinggi (layanan prioritas)."
  if "geography" in f:
    return "Perkuat layanan wilayah churn tinggi: CS proaktif, survei, promo lokal."
  if "hascrcard" in f:
    return "Penawaran kartu kredit: free annual fee, welcome point."
  if "creditscore" in f:
    return "Coaching kredit & produk aman (secured), notifikasi jatuh tempo."
  if "tenure" in f:
    return "Onboarding 90-hari: welcome call, edukasi fitur, promo transaksi awal."
  if "gender" in f:
    return "Fokus kebutuhan individual; hindari keputusan berbasis atribut sensitif."
  return "Outreach proaktif & evaluasi pengalaman nasabah."

topk = 8
insight = imp_group.head(topk).copy()
insight["Recommended_Action"] = insight["Base"].apply(recommend)

st.write("\n INSIGHT & AKSI (ringkas untuk manajemen)")
insight.to_string(index=False, formatters={"Importance":"{:.4f}".format})
